# PrivacyPilot v2.0 - Advanced RAG Pipeline

## Overview

PrivacyPilot is an advanced privacy policy analysis tool powered by hybrid Retrieval-Augmented Generation (RAG). It combines **token-aware chunking**, **dense vector search**, **BM25 lexical matching**, and **LLM reasoning** to provide comprehensive, evidence-based analysis of privacy policies.

## Key Features

### ğŸ¯ Hybrid Retrieval System
- **Dense Vector Search**: Semantic similarity using sentence-transformers
- **BM25 Lexical Search**: Keyword-based retrieval for precise matching
- **Weighted Fusion**: Configurable Î±/Î² weights (default: 0.4 BM25, 0.6 vector)

### ğŸ“Š Token-Aware Chunking
- **Smart Boundaries**: Respects section headers and sentence boundaries
- **Configurable Size**: Default 512 tokens with 100-token overlap (~20%)
- **Header Preservation**: Keeps section context intact

### ğŸ’¾ Intelligent Caching
- **SHA256-Based**: Deduplicated embedding storage
- **Persistent**: Survives sessions and reduces costs
- **Batch Processing**: Optimized for large documents

### ğŸ“ Structured Output
- **Markdown-First**: Human-readable reports
- **Evidence Tracking**: Every claim linked to source chunks
- **Confidence Scores**: High/medium/low confidence levels
- **Coverage Assessment**: Complete/partial/none coverage

### ğŸ§  LLM Integration
- **ChatGroq**: llama-3.3-70b-versatile (fast, accurate)
- **Structured Prompts**: JSON schema enforcement
- **Few-Shot Examples**: Improved consistency
- **Citation Requirements**: All answers must cite chunk IDs

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Privacy Policy â”‚
â”‚   (Raw Text)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Token-Aware    â”‚ â† 512 tokens, 100 overlap
â”‚    Chunking     â”‚ â† Header-aware splitting
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding Gen  â”‚ â† sentence-transformers/all-MiniLM-L6-v2
â”‚  + SHA256 Cache â”‚ â† Batch processing (32)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Chroma  â”‚  â”‚  BM25   â”‚  â”‚  Both   â”‚
    â”‚ (Dense) â”‚  â”‚(Lexical)â”‚  â”‚(Hybrid) â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚            â”‚            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Retrieval   â”‚ â† Top-K chunks
              â”‚  (Î± BM25 +   â”‚ â† Score fusion
              â”‚   Î² Vector)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  LLM Query   â”‚ â† ChatGroq
              â”‚  (Groq API)  â”‚ â† JSON output
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Markdown   â”‚
              â”‚    Report    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### 1. Clone Repository
```bash
git clone <repo-url>
cd PrivacyPilot
```

### 2. Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Setup Environment Variables
```bash
cp .env.example .env
# Edit .env and add your GROQ_API_KEY
```

## Quick Start

### Run Test Suite
```bash
python test_pipeline.py
```

### Analyze a Privacy Policy
```python
from pipeline import PrivacyPolicyPipeline
from langchain_groq import ChatGroq
import os

# Initialize LLM
llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    api_key=os.getenv("GROQ_API_KEY")
)

# Initialize pipeline
pipeline = PrivacyPolicyPipeline(
    chunk_size=512,
    overlap=100,
    use_hybrid=True,
    alpha=0.4,  # BM25 weight
    beta=0.6    # Vector weight
)

# Load your privacy policy
with open("policy.txt", "r") as f:
    policy_text = f.read()

# Run analysis
result = pipeline.analyze_policy(
    text=policy_text,
    url="https://company.com/privacy",
    company_name="Company Name",
    llm_client=llm
)

print(f"Report saved to: {result['report_path']}")
```

### Custom Queries
```python
# Index a document
pipeline.index_document(
    text=policy_text,
    url="https://company.com/privacy"
)

# Query specific questions
result = pipeline.query(
    question="How long is my data retained?",
    top_k=5
)

# Examine retrieved chunks
for chunk in result['chunks']:
    print(f"Score: {chunk['hybrid_score']:.3f}")
    print(f"Text: {chunk['text'][:200]}...")
```

## Pipeline Modules

### `pipeline/chunker.py`
- **TokenAwareChunker**: Token-based chunking with header awareness
- **chunk_privacy_policy()**: Convenience function

**Configuration:**
- `chunk_tokens`: Target chunk size (default: 512)
- `overlap_tokens`: Overlap between chunks (default: 100)
- `preserve_headers`: Keep headers with content (default: True)

### `pipeline/embedder.py`
- **CachedEmbedder**: Embedding generator with SHA256 caching
- **embed_privacy_chunks()**: Convenience function

**Configuration:**
- `model_name`: HuggingFace model (default: all-MiniLM-L6-v2)
- `cache_dir`: Cache directory (default: .embedding_cache)
- `batch_size`: Batch size for encoding (default: 32)

### `pipeline/indexer.py`
- **ChromaIndexer**: Vector store (local, fast)
- **BM25Retriever**: Lexical search (TF-IDF fallback)
- **HybridRetriever**: Combines both with weighted fusion

**Configuration:**
- `alpha`: BM25 weight (default: 0.4)
- `beta`: Vector weight (default: 0.6)
- `top_k`: Results to return (default: 10)

### `pipeline/reporter.py`
- **MarkdownReporter**: Generate structured reports
- **generate_privacy_report()**: Convenience function

**Features:**
- Executive summary
- Dimensional analysis
- Evidence tables
- Coverage assessment

### `pipeline/rag_pipeline.py`
- **PrivacyPolicyPipeline**: Main orchestrator
- End-to-end analysis workflow

## Prompt Engineering

The system uses structured prompts defined in `pipeline/prompt_template.json`:

### System Prompt
- Expert privacy analyst persona
- Evidence-based analysis requirements
- Citation enforcement

### Output Schema (JSON)
```json
{
  "answer": "string",
  "evidence": [
    {
      "chunk_id": "string",
      "quote": "string",
      "relevance": "high|medium|low"
    }
  ],
  "confidence": "high|medium|low",
  "coverage": "complete|partial|none"
}
```

### Analysis Dimensions
1. **Data Collection**: What, how, when
2. **Data Usage**: Purposes, profiling, marketing
3. **Data Sharing**: Third parties, transfers
4. **Data Retention**: Duration, deletion
5. **User Rights**: Access, deletion, portability
6. **Security**: Encryption, breach notification
7. **Children's Privacy**: Age limits, consent
8. **Policy Changes**: Notification procedures
9. **Legal Basis (GDPR)**: Consent, legitimate interest
10. **Contact & Complaints**: Support channels

## Configuration

### Chunking Strategy
```python
chunker = TokenAwareChunker(
    chunk_tokens=512,      # Adjust based on policy length
    overlap_tokens=100,    # 20% overlap recommended
    preserve_headers=True  # Keep section context
)
```

### Embedding Model
```python
embedder = CachedEmbedder(
    model_name="sentence-transformers/all-MiniLM-L6-v2",  # Fast, accurate
    # Alternatives:
    # - "all-mpnet-base-v2" (higher quality, slower)
    # - "paraphrase-multilingual-MiniLM-L12-v2" (multilingual)
)
```

### Hybrid Weights
```python
retriever = HybridRetriever(
    alpha=0.4,  # BM25 weight (keyword matching)
    beta=0.6    # Vector weight (semantic similarity)
)
# Adjust based on query type:
# - More keywords â†’ increase alpha
# - More semantic â†’ increase beta
```

## Performance

### Benchmarks (GitHub Terms of Service - 182KB)
- **Chunking**: ~0.15s (13 chunks, 344 tokens)
- **Embedding** (cached): ~0.01s
- **Embedding** (uncached): ~0.2s (batch of 13)
- **Indexing**: ~0.05s
- **Retrieval**: ~0.02s per query
- **LLM Call**: ~2-5s per question

**Total Analysis** (10 dimensions, 40 questions): ~3-5 minutes

## Troubleshooting

### "NotImplementedError: Event loop on Windows"
âœ… **Fixed** - Pipeline uses thread pool execution for async operations

### "Failed to send telemetry event"
â„¹ï¸ **Harmless** - Chroma telemetry warnings, doesn't affect functionality

### "LLM error: Extra data"
âš ï¸ **LLM Output** - Model sometimes returns explanatory text after JSON. Parser attempts extraction.

### "Permission denied: chroma.sqlite3"
ğŸ”’ **Windows File Lock** - Close ChromaDB connections before cleanup:
```python
pipeline.vector_store = None
```

## Project Structure

```
PrivacyPilot/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py           # Package exports
â”‚   â”œâ”€â”€ chunker.py            # Token-aware chunking
â”‚   â”œâ”€â”€ embedder.py           # Cached embedding generation
â”‚   â”œâ”€â”€ indexer.py            # Vector store & BM25
â”‚   â”œâ”€â”€ reporter.py           # Markdown report generation
â”‚   â”œâ”€â”€ rag_pipeline.py       # Main orchestrator
â”‚   â””â”€â”€ prompt_template.json  # Prompt engineering
â”œâ”€â”€ scrape/
â”‚   â”œâ”€â”€ scrape.py             # Web scraping (crawl4ai)
â”‚   â””â”€â”€ extract_link.py       # Google search
â”œâ”€â”€ reports/                  # Generated reports
â”œâ”€â”€ .embedding_cache/         # Embedding cache (SHA256)
â”œâ”€â”€ .chroma_db/              # Vector database
â”œâ”€â”€ test_pipeline.py         # Test suite
â”œâ”€â”€ main2.py                 # Legacy pipeline
â”œâ”€â”€ app.py                   # Streamlit app
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md               # This file
```

## Contributing

### Adding New Analysis Dimensions
1. Edit `pipeline/prompt_template.json`
2. Add dimension to `analysis_dimensions` array
3. Define questions for the dimension

### Improving Prompt Engineering
- Modify `system_prompt` for persona/requirements
- Add `few_shot_examples` for consistency
- Update `output_schema` for new fields

### Custom Retrievers
Implement retriever interface:
```python
class CustomRetriever:
    def search(self, query, query_embedding, top_k):
        # Return list of dicts with 'chunk_id', 'text', 'score'
        pass
```

## Citation

If you use PrivacyPilot in research:
```bibtex
@software{privacypilot2024,
  title = {PrivacyPilot: Advanced RAG-Based Privacy Policy Analysis},
  author = {Your Name},
  year = {2024},
  url = {https://github.com/yourusername/PrivacyPilot}
}
```

## License

MIT License - See LICENSE file

## Acknowledgments

- **crawl4ai**: Web scraping framework
- **sentence-transformers**: Embedding models
- **ChromaDB**: Vector database
- **Groq**: LLM inference
- **LangChain**: LLM orchestration

---

**Version**: 2.0  
**Last Updated**: 2024  
**Status**: âœ… Production Ready
